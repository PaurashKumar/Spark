{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e4bcec",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-29T11:37:38.870328Z",
     "iopub.status.busy": "2023-10-29T11:37:38.869883Z",
     "iopub.status.idle": "2023-10-29T11:38:30.985874Z",
     "shell.execute_reply": "2023-10-29T11:38:30.984544Z"
    },
    "papermill": {
     "duration": 52.127478,
     "end_time": "2023-10-29T11:38:30.989024",
     "exception": false,
     "start_time": "2023-10-29T11:37:38.861546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=f1c3dfe399a9ebd99fa452f878f3e1138af75601b6c0d044f0df539a2f5cbeb5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aac8db4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T11:38:31.019179Z",
     "iopub.status.busy": "2023-10-29T11:38:31.018804Z",
     "iopub.status.idle": "2023-10-29T11:38:31.108154Z",
     "shell.execute_reply": "2023-10-29T11:38:31.107307Z"
    },
    "papermill": {
     "duration": 0.10762,
     "end_time": "2023-10-29T11:38:31.110751",
     "exception": false,
     "start_time": "2023-10-29T11:38:31.003131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58bbea",
   "metadata": {
    "papermill": {
     "duration": 0.013468,
     "end_time": "2023-10-29T11:38:31.137967",
     "exception": false,
     "start_time": "2023-10-29T11:38:31.124499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## If we want to use RDD we need to create Spark Context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4feb88e",
   "metadata": {
    "papermill": {
     "duration": 0.013236,
     "end_time": "2023-10-29T11:38:31.164691",
     "exception": false,
     "start_time": "2023-10-29T11:38:31.151455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " Way first to create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246e6b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T11:38:31.194185Z",
     "iopub.status.busy": "2023-10-29T11:38:31.192868Z",
     "iopub.status.idle": "2023-10-29T11:38:31.197679Z",
     "shell.execute_reply": "2023-10-29T11:38:31.196947Z"
    },
    "papermill": {
     "duration": 0.021517,
     "end_time": "2023-10-29T11:38:31.199686",
     "exception": false,
     "start_time": "2023-10-29T11:38:31.178169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#conf=SparkConf().setAppName(\"RDDAction\").setMaster(\"local\")\n",
    "#sc=SparkContext(conf=conf)\n",
    "#sc.getConf.getAll()\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92039198",
   "metadata": {
    "papermill": {
     "duration": 0.013235,
     "end_time": "2023-10-29T11:38:31.226392",
     "exception": false,
     "start_time": "2023-10-29T11:38:31.213157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Second way to create SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c9d06a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T11:38:31.255376Z",
     "iopub.status.busy": "2023-10-29T11:38:31.254684Z",
     "iopub.status.idle": "2023-10-29T11:38:36.815390Z",
     "shell.execute_reply": "2023-10-29T11:38:36.814014Z"
    },
    "papermill": {
     "duration": 5.579371,
     "end_time": "2023-10-29T11:38:36.819246",
     "exception": false,
     "start_time": "2023-10-29T11:38:31.239875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/29 11:38:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc=pyspark.SparkContext(appName=\"RDD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d25706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T11:38:36.853405Z",
     "iopub.status.busy": "2023-10-29T11:38:36.852470Z",
     "iopub.status.idle": "2023-10-29T11:38:38.032676Z",
     "shell.execute_reply": "2023-10-29T11:38:38.031607Z"
    },
    "papermill": {
     "duration": 1.199561,
     "end_time": "2023-10-29T11:38:38.035648",
     "exception": false,
     "start_time": "2023-10-29T11:38:36.836087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chupke chupke rat din', 'hume tumse pyaar kitna', 'rafta rafta vo mere ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs=sc.parallelize(['chupke chupke rat din','hume tumse pyaar kitna','rafta rafta vo mere '])\n",
    "songs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8348910e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-29T11:38:38.078308Z",
     "iopub.status.busy": "2023-10-29T11:38:38.077333Z",
     "iopub.status.idle": "2023-10-29T11:38:39.852401Z",
     "shell.execute_reply": "2023-10-29T11:38:39.851342Z"
    },
    "papermill": {
     "duration": 1.79997,
     "end_time": "2023-10-29T11:38:39.855466",
     "exception": false,
     "start_time": "2023-10-29T11:38:38.055496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CHUPKE CHUPKE RAT DIN', 'HUME TUMSE PYAAR KITNA', 'RAFTA RAFTA VO MERE ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_song=songs.map(lambda a:a.upper())\n",
    "dist_song.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3460f",
   "metadata": {
    "papermill": {
     "duration": 0.019634,
     "end_time": "2023-10-29T11:38:39.896487",
     "exception": false,
     "start_time": "2023-10-29T11:38:39.876853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 67.092365,
   "end_time": "2023-10-29T11:38:42.531733",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-29T11:37:35.439368",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
